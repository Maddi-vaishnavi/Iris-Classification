import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.neighbors import KNeighborsClassifier

# Change directory
os.chdir("D:/B.tech{CS-DS}/OASIS INFOBYTE/TASKS--/Task_1")

# Load data
data = pd.read_csv('Iris.csv')

# Display first and last few rows
print(data.head(5))
print("---------------------------------------------------------")
print(data.tail(2))
print("---------------------------------------------------------")
# Check information about the dataset
print(data.info())
print("---------------------------------------------------------")
# Check missing values
missing = data.isna().sum()
print(missing)
print("---------------------------------------------------------")
# Statistical summary
print(data.describe())
print("---------------------------------------------------------")
# Visualize the whole dataset
sns.pairplot(data, hue='Species')
print("---------------------------------------------------------")
# Convert 'Species' column to categorical
data['Species'] = data['Species'].astype('category')

# Separate features and target
X = data.drop('Species', axis=1)
y = data['Species']

# Encode target labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Print the mapping of original labels to encoded labels
print("Mapping of original labels to encoded labels:")
for original_label, encoded_label in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)):
    print(original_label, "-->", encoded_label)
print("---------------------------------------------------------")
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initialize and train the KNeighborsClassifier
model = KNeighborsClassifier()
model.fit(X_train, y_train)

# Predict on test set
y_pred = model.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy on test data:", accuracy)
print("---------------------------------------------------------")
# Predict on train set
y_pred_train = model.predict(X_train)
print(f"Accuracy of Train data: {accuracy_score(y_train, y_pred_train):.4f}")
print("---------------------------------------------------------")
# Visualize predictions
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap='viridis')
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.title('Predictions on Test Data')
plt.show()

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Initialize and configure k-fold cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Initialize list to store accuracies
accuracies = []

# Perform k-fold cross-validation
for fold, (train_index, test_index) in enumerate(kf.split(X_scaled)):
    X_train, X_test = X_scaled[train_index], X_scaled[test_index]
    y_train, y_test = y_encoded[train_index], y_encoded[test_index]

    model = KNeighborsClassifier()
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)
    print(f"Fold {fold+1} Accuracy: {accuracy}")

# Calculate and print mean accuracy
mean_accuracy = np.mean(accuracies)
print(f"Mean Accuracy: {mean_accuracy}")
